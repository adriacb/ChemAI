{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd4ac20-1d16-41ca-bb5e-e317f3cf5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "sys.path.append('..')\n",
    "from llm.models.mha import MultiHeadAttention\n",
    "from llm.tools import CustomSchedule\n",
    "from llm.models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101878d",
   "metadata": {},
   "source": [
    "## MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a0e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # Ensure this is an integer\n",
    "encoder_sequence = 60\n",
    "dim = 512\n",
    "y = torch.rand(batch_size, encoder_sequence, dim)\n",
    "\n",
    "mha = MultiHeadAttention(dim, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27dc9717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.0062,  0.0603,  0.1107,  ...,  0.0973, -0.3836, -0.0327],\n",
      "         [-0.0054,  0.0603,  0.1108,  ...,  0.0961, -0.3839, -0.0324],\n",
      "         [-0.0066,  0.0600,  0.1110,  ...,  0.0965, -0.3835, -0.0322],\n",
      "         ...,\n",
      "         [-0.0057,  0.0603,  0.1108,  ...,  0.0966, -0.3842, -0.0323],\n",
      "         [-0.0057,  0.0603,  0.1113,  ...,  0.0968, -0.3841, -0.0326],\n",
      "         [-0.0061,  0.0600,  0.1105,  ...,  0.0965, -0.3845, -0.0329]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[[0.0198, 0.0155, 0.0147,  ..., 0.0158, 0.0163, 0.0163],\n",
      "          [0.0207, 0.0154, 0.0151,  ..., 0.0169, 0.0162, 0.0160],\n",
      "          [0.0203, 0.0158, 0.0151,  ..., 0.0167, 0.0159, 0.0151],\n",
      "          ...,\n",
      "          [0.0193, 0.0155, 0.0150,  ..., 0.0166, 0.0160, 0.0168],\n",
      "          [0.0186, 0.0155, 0.0152,  ..., 0.0172, 0.0163, 0.0157],\n",
      "          [0.0193, 0.0155, 0.0149,  ..., 0.0171, 0.0167, 0.0166]],\n",
      "\n",
      "         [[0.0169, 0.0179, 0.0166,  ..., 0.0170, 0.0164, 0.0166],\n",
      "          [0.0175, 0.0177, 0.0176,  ..., 0.0175, 0.0162, 0.0174],\n",
      "          [0.0176, 0.0180, 0.0171,  ..., 0.0173, 0.0171, 0.0177],\n",
      "          ...,\n",
      "          [0.0167, 0.0171, 0.0172,  ..., 0.0173, 0.0173, 0.0178],\n",
      "          [0.0174, 0.0176, 0.0162,  ..., 0.0170, 0.0170, 0.0179],\n",
      "          [0.0169, 0.0168, 0.0171,  ..., 0.0163, 0.0173, 0.0168]],\n",
      "\n",
      "         [[0.0174, 0.0174, 0.0170,  ..., 0.0182, 0.0174, 0.0174],\n",
      "          [0.0173, 0.0173, 0.0166,  ..., 0.0177, 0.0170, 0.0162],\n",
      "          [0.0168, 0.0176, 0.0163,  ..., 0.0179, 0.0172, 0.0175],\n",
      "          ...,\n",
      "          [0.0178, 0.0177, 0.0170,  ..., 0.0179, 0.0167, 0.0172],\n",
      "          [0.0164, 0.0176, 0.0168,  ..., 0.0171, 0.0175, 0.0170],\n",
      "          [0.0174, 0.0174, 0.0167,  ..., 0.0178, 0.0171, 0.0171]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0155, 0.0163, 0.0177,  ..., 0.0160, 0.0161, 0.0183],\n",
      "          [0.0153, 0.0165, 0.0180,  ..., 0.0164, 0.0170, 0.0182],\n",
      "          [0.0160, 0.0166, 0.0178,  ..., 0.0155, 0.0169, 0.0186],\n",
      "          ...,\n",
      "          [0.0154, 0.0163, 0.0173,  ..., 0.0158, 0.0163, 0.0184],\n",
      "          [0.0150, 0.0171, 0.0173,  ..., 0.0162, 0.0170, 0.0179],\n",
      "          [0.0151, 0.0167, 0.0185,  ..., 0.0165, 0.0174, 0.0179]],\n",
      "\n",
      "         [[0.0143, 0.0165, 0.0176,  ..., 0.0174, 0.0156, 0.0192],\n",
      "          [0.0150, 0.0172, 0.0166,  ..., 0.0174, 0.0153, 0.0181],\n",
      "          [0.0146, 0.0172, 0.0176,  ..., 0.0172, 0.0155, 0.0190],\n",
      "          ...,\n",
      "          [0.0153, 0.0160, 0.0173,  ..., 0.0168, 0.0159, 0.0181],\n",
      "          [0.0149, 0.0161, 0.0161,  ..., 0.0170, 0.0154, 0.0187],\n",
      "          [0.0155, 0.0162, 0.0168,  ..., 0.0169, 0.0151, 0.0178]],\n",
      "\n",
      "         [[0.0172, 0.0155, 0.0180,  ..., 0.0163, 0.0188, 0.0161],\n",
      "          [0.0167, 0.0157, 0.0188,  ..., 0.0164, 0.0177, 0.0161],\n",
      "          [0.0168, 0.0149, 0.0181,  ..., 0.0169, 0.0182, 0.0165],\n",
      "          ...,\n",
      "          [0.0161, 0.0150, 0.0180,  ..., 0.0173, 0.0176, 0.0161],\n",
      "          [0.0166, 0.0154, 0.0178,  ..., 0.0162, 0.0183, 0.0153],\n",
      "          [0.0166, 0.0153, 0.0180,  ..., 0.0173, 0.0172, 0.0159]]]],\n",
      "       grad_fn=<SoftmaxBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "output = mha(y, y, y, mask=None)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cf5ad",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens = [13347, 0, 3092, 836, 374, 7011, 383, 355, 13]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f7cac",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a331c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(seed=1234)\n",
    "# PARAMETERS\n",
    "vocab_size = max(tokens) + 1      # number of classes to predict\n",
    "e_dim = 5                         # size of the vector representing each token\n",
    "context = len(tokens)             # number of tokens in the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f81074e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = CustomSchedule(512)\n",
    "optimizer = optim.Adam(mha.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# mean loss\n",
    "train_loss = nn.MSELoss()\n",
    "# Sparse categorical accuracy\n",
    "train_acc = nn.CrossEntropyLoss()\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=6,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    dff=2048,\n",
    "    input_vocab_size=8500,\n",
    "    target_vocab_size=8000,\n",
    "    max_pe_input=1000,\n",
    "    max_pe_target=1000,\n",
    "    rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd2c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_step_signature = [\n",
    "    dict(\n",
    "        input=torch.Tensor,\n",
    "        target=torch.Tensor\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a89d629",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, acc\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Test forward pass\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer(\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39my, target\u001b[38;5;241m=\u001b[39my, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enc_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, look_ahead_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dec_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Test training step\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cabe\\Documents\\repos\\ChemAI\\notebooks\\..\\llm\\models\\transformer.py:201\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[1;34m(self, input, target, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m    181\u001b[0m              \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor, \n\u001b[0;32m    182\u001b[0m              target: torch\u001b[38;5;241m.\u001b[39mTensor, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m              look_ahead_mask: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    186\u001b[0m              dec_padding_mask: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    Forward pass of the transformer model.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    - x: torch.Tensor of shape (batch_size, target_seq_len, target_vocab_size)\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;28minput\u001b[39m, training, enc_padding_mask)\n\u001b[0;32m    203\u001b[0m     dec_output, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(target, enc_output, training, look_ahead_mask, dec_padding_mask)\n\u001b[0;32m    205\u001b[0m     final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(dec_output)\n",
      "File \u001b[1;32mc:\\Users\\cabe\\Documents\\repos\\ChemAI\\notebooks\\..\\llm\\models\\transformer.py:67\u001b[0m, in \u001b[0;36mEncoder.__call__\u001b[1;34m(self, input, training, mask)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mForward pass of the encoder.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m- x: torch.Tensor of shape (batch_size, input_seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     68\u001b[0m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     69\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(torch\u001b[38;5;241m.\u001b[39marange(seq_len)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\cabe\\anaconda3\\envs\\MLChem\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cabe\\anaconda3\\envs\\MLChem\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cabe\\anaconda3\\envs\\MLChem\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32mc:\\Users\\cabe\\anaconda3\\envs\\MLChem\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df01520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLChem",
   "language": "python",
   "name": "mlchem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
